{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sampling\" data-toc-modified-id=\"Sampling-1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sampling</a></span></li><li><span><a href=\"#Code-Snippets\" data-toc-modified-id=\"Code-Snippets-2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code Snippets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimating-uniqueness-of-a-label-[4.1]\" data-toc-modified-id=\"Estimating-uniqueness-of-a-label-[4.1]-2.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Estimating uniqueness of a label [4.1]</a></span></li><li><span><a href=\"#Estimating-the-average-uniqueness-of-a-label-[4.2]\" data-toc-modified-id=\"Estimating-the-average-uniqueness-of-a-label-[4.2]-2.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Estimating the average uniqueness of a label [4.2]</a></span></li><li><span><a href=\"#Sequential-Bootstrap-[4.5.2]\" data-toc-modified-id=\"Sequential-Bootstrap-[4.5.2]-2.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Sequential Bootstrap [4.5.2]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Indicator-Matrix-[4.3]\" data-toc-modified-id=\"Build-Indicator-Matrix-[4.3]-2.3.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Build Indicator Matrix [4.3]</a></span></li><li><span><a href=\"#Compute-average-uniqueness-[4.4]\" data-toc-modified-id=\"Compute-average-uniqueness-[4.4]-2.3.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Compute average uniqueness [4.4]</a></span></li><li><span><a href=\"#return-sample-from-sequential-bootstrap-[4.5]\" data-toc-modified-id=\"return-sample-from-sequential-bootstrap-[4.5]-2.3.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>return sample from sequential bootstrap [4.5]</a></span></li></ul></li><li><span><a href=\"#Determination-of-sample-weight-by-absolute-return-attribution-[4.10]\" data-toc-modified-id=\"Determination-of-sample-weight-by-absolute-return-attribution-[4.10]-2.4\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Determination of sample weight by absolute return attribution [4.10]</a></span></li><li><span><a href=\"#Implementation-of-Time-Decay-Factors-[4.11]\" data-toc-modified-id=\"Implementation-of-Time-Decay-Factors-[4.11]-2.5\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Implementation of Time-Decay Factors [4.11]</a></span></li></ul></li><li><span><a href=\"#Example-of-Sequential-Bootstrap-[4.6]\" data-toc-modified-id=\"Example-of-Sequential-Bootstrap-[4.6]-3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example of Sequential Bootstrap [4.6]</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-4\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exercises</a></span><ul class=\"toc-item\"><li><span><a href=\"#[4.1]\" data-toc-modified-id=\"[4.1]-4.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>[4.1]</a></span><ul class=\"toc-item\"><li><span><a href=\"#(a)-compute-a-t1-series-using-dollar-bars-derived-from-dataset\" data-toc-modified-id=\"(a)-compute-a-t1-series-using-dollar-bars-derived-from-dataset-4.1.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>(a) compute a t1 series using dollar bars derived from dataset</a></span></li><li><span><a href=\"#(b)-Apply-the-function-mpNumCoEvents-to-compute-the-number-of-overlapping-outcomes-at-each-point-in-time.\" data-toc-modified-id=\"(b)-Apply-the-function-mpNumCoEvents-to-compute-the-number-of-overlapping-outcomes-at-each-point-in-time.-4.1.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>(b) Apply the function <code>mpNumCoEvents</code> to compute the number of overlapping outcomes at each point in time.</a></span></li><li><span><a href=\"#(c)-Plot-the-time-series-of-number-of-concurrent-labels-on-primary-axis-and-time-series-of-exponentially-weighted-moving-standard-deviation-of-returns-on-secondary-axis\" data-toc-modified-id=\"(c)-Plot-the-time-series-of-number-of-concurrent-labels-on-primary-axis-and-time-series-of-exponentially-weighted-moving-standard-deviation-of-returns-on-secondary-axis-4.1.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>(c) Plot the time series of number of concurrent labels on primary axis and time series of exponentially weighted moving standard deviation of returns on secondary axis</a></span></li><li><span><a href=\"#(d)-Produce-a-scatterplot-of-the-number-of-concurrent-labels-(x-axis)-and-the-exponentially-weighted-moving-std-dev-of-returns-(y-axis).\" data-toc-modified-id=\"(d)-Produce-a-scatterplot-of-the-number-of-concurrent-labels-(x-axis)-and-the-exponentially-weighted-moving-std-dev-of-returns-(y-axis).-4.1.4\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>(d) Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially weighted moving std dev of returns (y-axis).</a></span></li></ul></li><li><span><a href=\"#[4.2]-Using-the-function-mpSampleTW-compute-the-avg-uniqueness-of-each-label.-What-is-the-first-order-serial-correlation,-AR(1)-of-this-time-series?-Is-it-statistically-significant?-Why?\" data-toc-modified-id=\"[4.2]-Using-the-function-mpSampleTW-compute-the-avg-uniqueness-of-each-label.-What-is-the-first-order-serial-correlation,-AR(1)-of-this-time-series?-Is-it-statistically-significant?-Why?-4.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>[4.2] Using the function <code>mpSampleTW</code> compute the avg uniqueness of each label. What is the first-order serial correlation, <code>AR(1)</code> of this time series? Is it statistically significant? Why?</a></span></li><li><span><a href=\"#[4.3]-Fit-a-random-forest-to-a-financial-dataset-where-$I^{-1}\\sum_{i=1}^{I}\\bar-u-\\ll-1$\" data-toc-modified-id=\"[4.3]-Fit-a-random-forest-to-a-financial-dataset-where-$I^{-1}\\sum_{i=1}^{I}\\bar-u-\\ll-1$-4.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>[4.3] Fit a random forest to a financial dataset where $I^{-1}\\sum_{i=1}^{I}\\bar u \\ll 1$</a></span><ul class=\"toc-item\"><li><span><a href=\"#(a)-What-is-the-mean-out-of-bag-accuracy?\" data-toc-modified-id=\"(a)-What-is-the-mean-out-of-bag-accuracy?-4.3.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>(a) What is the mean out of bag accuracy?</a></span></li><li><span><a href=\"#(b)-What-is-the-mean-accuracy-of-k-fold-cross-validation-(without-shuffling)-on-the-same-dataset?\" data-toc-modified-id=\"(b)-What-is-the-mean-accuracy-of-k-fold-cross-validation-(without-shuffling)-on-the-same-dataset?-4.3.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>(b) What is the mean accuracy of k-fold cross-validation (without shuffling) on the same dataset?</a></span></li><li><span><a href=\"#Why-is-out-of-bag-accuracy-so-much-higher-than-cross-validation-accuracy?-Which-one-is-more-correct-/-less-biased?-What-is-the-source-of-this-bias?\" data-toc-modified-id=\"Why-is-out-of-bag-accuracy-so-much-higher-than-cross-validation-accuracy?-Which-one-is-more-correct-/-less-biased?-What-is-the-source-of-this-bias?-4.3.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which one is more correct / less biased? What is the source of this bias?</a></span></li></ul></li><li><span><a href=\"#Modify-the-code-in-Section-4.7-to-apply-an-exponential-time-decay-factor\" data-toc-modified-id=\"Modify-the-code-in-Section-4.7-to-apply-an-exponential-time-decay-factor-4.4\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Modify the code in Section 4.7 to apply an exponential time-decay factor</a></span></li><li><span><a href=\"#Consider-you-have-applied-meta-labels-to-events-determined-by-a-trend-following-model.-Suppose-2/3-of-labels-are-0-and-1/3-are-1.\" data-toc-modified-id=\"Consider-you-have-applied-meta-labels-to-events-determined-by-a-trend-following-model.-Suppose-2/3-of-labels-are-0-and-1/3-are-1.-4.5\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Consider you have applied meta-labels to events determined by a trend-following model. Suppose <code>2/3</code> of labels are <code>0</code> and <code>1/3</code> are <code>1</code>.</a></span><ul class=\"toc-item\"><li><span><a href=\"#(a)-What-happens-if-you-fit-a-classifier-without-balancing-class-weights?\" data-toc-modified-id=\"(a)-What-happens-if-you-fit-a-classifier-without-balancing-class-weights?-4.5.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>(a) What happens if you fit a classifier without balancing class weights?</a></span></li><li><span><a href=\"#(b)-A-label-1-means-true-positive-and-a-label-0-means-a-false-positive.-By-applying-balanced-class-weights,-we-are-forcing-the-classifier-to-pay-more-attention-to-the-true-positives,-and-less-attention-to-the-false-positives.-Why-does-that-make-sense?\" data-toc-modified-id=\"(b)-A-label-1-means-true-positive-and-a-label-0-means-a-false-positive.-By-applying-balanced-class-weights,-we-are-forcing-the-classifier-to-pay-more-attention-to-the-true-positives,-and-less-attention-to-the-false-positives.-Why-does-that-make-sense?-4.5.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>(b) A label <code>1</code> means true positive and a label <code>0</code> means a false positive. By applying balanced class weights, we are forcing the classifier to pay more attention to the true positives, and less attention to the false positives. Why does that make sense?</a></span></li><li><span><a href=\"#(c)-What-is-the-distribution-of-the-predicted-labels,-before-and-after-applying-balanced-class-weights?\" data-toc-modified-id=\"(c)-What-is-the-distribution-of-the-predicted-labels,-before-and-after-applying-balanced-class-weights?-4.5.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>(c) What is the distribution of the predicted labels, before and after applying balanced class weights?</a></span></li></ul></li><li><span><a href=\"#Update-the-draw-probabilities-for-the-final-draw-in-section-4.5.3.\" data-toc-modified-id=\"Update-the-draw-probabilities-for-the-final-draw-in-section-4.5.3.-4.6\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Update the draw probabilities for the final draw in section <code>4.5.3.</code></a></span></li><li><span><a href=\"#In-Section-4.5.3-suppose-that-number-2-is-picked-again-in-the-second-draw.-Waht-would-be-the-updated-probabilities-for-the-third-draw?\" data-toc-modified-id=\"In-Section-4.5.3-suppose-that-number-2-is-picked-again-in-the-second-draw.-Waht-would-be-the-updated-probabilities-for-the-third-draw?-4.7\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>In Section <code>4.5.3</code> suppose that number 2 is picked again in the second draw. Waht would be the updated probabilities for the third draw?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:37.708037Z",
     "start_time": "2018-10-18T22:50:33.356022Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import standard libs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu'\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count \n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import pymc3 as pm\n",
    "import numba as nb\n",
    "import math\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import plotnine as pn\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "#plt.rcParams['font.family'] = 'DejaVu Sans Mono'\n",
    "plt.rcParams['font.size'] = 9.5\n",
    "plt.rcParams['font.weight'] = 'medium'\n",
    "plt.rcParams['figure.figsize'] = 10,7\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "# import util libs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import missingno as msno\n",
    "from src.utils.utils import *\n",
    "import src.features.bars as brs\n",
    "import src.features.snippets as snp\n",
    "\n",
    "import copyreg, types\n",
    "copyreg.pickle(types.MethodType,snp._pickle_method,snp._unpickle_method)\n",
    "RANDOM_STATE = 777\n",
    "\n",
    "pdir = get_relative_project_dir('Adv_Fin_ML_Exercises')\n",
    "data_dir = pdir/'data'/'processed'\n",
    "\n",
    "print()\n",
    "%watermark -p pandas,numpy,numba,pymc3,sklearn,statsmodels,scipy,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating uniqueness of a label [4.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:37.991677Z",
     "start_time": "2018-10-18T22:50:37.710063Z"
    }
   },
   "outputs": [],
   "source": [
    "def mpNumCoEvents(closeIdx,t1,molecule):\n",
    "    '''\n",
    "    Compute the number of concurrent events per bar.\n",
    "    +molecule[0] is the date of the first event on which the weight will be computed\n",
    "    +molecule[-1] is the date of the last event on which the weight will be computed\n",
    "    \n",
    "    Any event that starts before t1[modelcule].max() impacts the count.\n",
    "    '''\n",
    "    #1) find events that span the period [molecule[0],molecule[-1]]\n",
    "    t1=t1.fillna(closeIdx[-1]) # unclosed events still must impact other weights\n",
    "    t1=t1[t1>=molecule[0]] # events that end at or after molecule[0]\n",
    "    t1=t1.loc[:t1[molecule].max()] # events that start at or before t1[molecule].max()\n",
    "    #2) count events spanning a bar\n",
    "    iloc=closeIdx.searchsorted(np.array([t1.index[0],t1.max()]))\n",
    "    count=pd.Series(0,index=closeIdx[iloc[0]:iloc[1]+1])\n",
    "    for tIn,tOut in t1.iteritems():count.loc[tIn:tOut]+=1.\n",
    "    return count.loc[molecule[0]:t1[molecule].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the average uniqueness of a label [4.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:38.249172Z",
     "start_time": "2018-10-18T22:50:37.993222Z"
    }
   },
   "outputs": [],
   "source": [
    "def mpSampleTW(t1,numCoEvents,molecule):\n",
    "    # Derive avg. uniqueness over the events lifespan\n",
    "    wght=pd.Series(index=molecule)\n",
    "    for tIn,tOut in t1.loc[wght.index].iteritems():\n",
    "        wght.loc[tIn]=(1./numCoEvents.loc[tIn:tOut]).mean()\n",
    "    return wght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Bootstrap [4.5.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Indicator Matrix [4.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:38.502666Z",
     "start_time": "2018-10-18T22:50:38.251167Z"
    }
   },
   "outputs": [],
   "source": [
    "def getIndMatrix(barIx,t1):\n",
    "    # Get Indicator matrix\n",
    "    indM=(pd.DataFrame(0,index=barIx,columns=range(t1.shape[0])))\n",
    "    for i,(t0,t1) in enumerate(t1.iteritems()):indM.loc[t0:t1,i]=1.\n",
    "    return indM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average uniqueness [4.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:38.762572Z",
     "start_time": "2018-10-18T22:50:38.504349Z"
    }
   },
   "outputs": [],
   "source": [
    "def getAvgUniqueness(indM):\n",
    "    # Average uniqueness from indicator matrix\n",
    "    c=indM.sum(axis=1) # concurrency\n",
    "    u=indM.div(c,axis=0) # uniqueness\n",
    "    avgU=u[u>0].mean() # avg. uniqueness\n",
    "    return avgU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### return sample from sequential bootstrap [4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:39.027953Z",
     "start_time": "2018-10-18T22:50:38.764091Z"
    }
   },
   "outputs": [],
   "source": [
    "def seqBootstrap(indM,sLength=None):\n",
    "    # Generate a sample via sequential bootstrap\n",
    "    if sLength is None:sLength=indM.shape[1]\n",
    "    phi=[]\n",
    "    while len(phi)<sLength:\n",
    "        avgU=pd.Series()\n",
    "        for i in indM:\n",
    "            indM_=indM[phi+[i]] # reduce indM\n",
    "            avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]\n",
    "        prob=avgU/avgU.sum() # draw prob\n",
    "        phi+=[np.random.choice(indM.columns,p=prob)]\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determination of sample weight by absolute return attribution [4.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:39.280316Z",
     "start_time": "2018-10-18T22:50:39.029708Z"
    }
   },
   "outputs": [],
   "source": [
    "def mpSampleW(t1,numCoEvents,close,molecule):\n",
    "    # Derive sample weight by return attribution\n",
    "    ret=np.log(close).diff() # log-returns, so that they are additive\n",
    "    wght=pd.Series(index=molecule)\n",
    "    for tIn,tOut in t1.loc[wght.index].iteritems():\n",
    "        wght.loc[tIn]=(ret.loc[tIn:tOut]/numCoEvents.loc[tIn:tOut]).sum()\n",
    "    return wght.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Time-Decay Factors [4.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:39.547944Z",
     "start_time": "2018-10-18T22:50:39.282243Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTimeDecay(tW,clfLastW=1.):\n",
    "    # apply piecewise-linear decay to observed uniqueness (tW)\n",
    "    # newest observation gets weight=1, oldest observation gets weight=clfLastW\n",
    "    clfW=tW.sort_index().cumsum()\n",
    "    if clfLastW>=0: slope=(1.-clfLastW)/clfW.iloc[-1]\n",
    "    else: slope=1./((clfLastW+1)*clfW.iloc[-1])\n",
    "    const=1.-slope*clfW.iloc[-1]\n",
    "    clfW=const+slope*clfW\n",
    "    clfW[clfW<0]=0\n",
    "    print(const,slope)\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Sequential Bootstrap [4.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:50:39.859262Z",
     "start_time": "2018-10-18T22:50:39.549658Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.random.seed(12121) # fix seed as results are unstable\n",
    "    t1=pd.Series([2,3,5],index=[0,2,4]) # t0,t1 for each feature obs\n",
    "    barIx=range(t1.max()+1) # index of bars\n",
    "    indM=snp.getIndMatrix(barIx,t1)\n",
    "    phi_random=np.random.choice(indM.columns,size=indM.shape[1])\n",
    "    print(phi_random)\n",
    "    print(f'Standard uniqueness: {snp.getAvgUniqueness(indM[phi_random]).mean():.4f}')\n",
    "    phi_seq=snp.seqBootstrap(indM)\n",
    "    print(phi_seq)\n",
    "    print(f'Sequential uniqueness: {snp.getAvgUniqueness(indM[phi_seq]).mean():.4f}')\n",
    "    \n",
    "main()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:01.346200Z",
     "start_time": "2018-10-18T22:50:39.860803Z"
    }
   },
   "outputs": [],
   "source": [
    "def dask_resample(ser, freq='L'):\n",
    "    dds = dd.from_pandas(ser, chunksize=len(ser)//100)\n",
    "    tdf = (dds\n",
    "           .resample(freq)\n",
    "           .mean()\n",
    "           .dropna()\n",
    "          ).compute()\n",
    "    return tdf\n",
    "\n",
    "infp=Path(data_dir/'clean_IVE_fut_prices.parquet')\n",
    "df = pd.read_parquet(infp)\n",
    "cprint(df)\n",
    "\n",
    "dv_rs = dask_resample(df, '1s')\n",
    "cprint(dv_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.1]\n",
    "\n",
    "### (a) compute a t1 series using dollar bars derived from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:19.330288Z",
     "start_time": "2018-10-18T22:52:01.348289Z"
    }
   },
   "outputs": [],
   "source": [
    "dbars = brs.dollar_bar_df(dv_rs, 'dv', 1_000_000)\n",
    "cprint(dbars)\n",
    "\n",
    "close = dbars.price.copy()\n",
    "dailyVol = snp.getDailyVol(close)\n",
    "cprint(dailyVol)\n",
    "\n",
    "tEvents = snp.getTEvents(close,h=dailyVol.mean())\n",
    "print(tEvents)\n",
    "#cprint(tEvents)\n",
    "\n",
    "t1 = snp.addVerticalBarrier(tEvents, close)\n",
    "cprint(t1)\n",
    "\n",
    "# select profit taking stoploss factor\n",
    "ptsl = [1,1]\n",
    "# target is dailyVol computed earlier\n",
    "target=dailyVol\n",
    "# select minRet\n",
    "minRet = 0.005\n",
    "# get cpu count - 1\n",
    "cpus = cpu_count() - 1\n",
    "\n",
    "events = snp.getEvents(close,tEvents,ptsl,target,minRet,cpus,t1=t1)\n",
    "cprint(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Apply the function `mpNumCoEvents` to compute the number of overlapping outcomes at each point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:21.823919Z",
     "start_time": "2018-10-18T22:52:19.331794Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Example\n",
    "numCoEvents = snp.mpPandasObj(snp.mpNumCoEvents,('molecule',events.index),                         \n",
    "                              cpus,closeIdx=close.index,t1=events['t1'])\n",
    "numCoEvents = numCoEvents.loc[~numCoEvents.index.duplicated(keep='last')]\n",
    "numCoEvents = numCoEvents.reindex(close.index).fillna(0)\n",
    "out=pd.DataFrame()\n",
    "out['tW'] = snp.mpPandasObj(snp.mpSampleTW,('molecule',events.index),\n",
    "                            cpus,t1=events['t1'],numCoEvents=numCoEvents)\n",
    "## example ##\n",
    "out['w']=snp.mpPandasObj(snp.mpSampleW,('molecule',events.index),cpus,\n",
    "                         t1=events['t1'],numCoEvents=numCoEvents,close=close)\n",
    "out['w']*=out.shape[0]/out['w'].sum()\n",
    "\n",
    "cprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:22.821171Z",
     "start_time": "2018-10-18T22:52:21.825889Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "out.reset_index(drop=True).plot(subplots=True, alpha=0.5, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Plot the time series of number of concurrent labels on primary axis and time series of exponentially weighted moving standard deviation of returns on secondary axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:23.067750Z",
     "start_time": "2018-10-18T22:52:22.822502Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coEvents_std = (\n",
    "    pd.DataFrame()\n",
    "    .assign(\n",
    "        numCoEvents = numCoEvents.reset_index(drop=True),\n",
    "        std = brs.returns(dbars.price).ewm(50).std().reset_index(drop=True))\n",
    ")\n",
    "cprint(coEvents_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:23.632764Z",
     "start_time": "2018-10-18T22:52:23.069377Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "coEvents_std.numCoEvents.plot(legend=True, ax=ax)\n",
    "coEvents_std['std'].plot(secondary_y=True, legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially weighted moving std dev of returns (y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:26.270153Z",
     "start_time": "2018-10-18T22:52:23.634114Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "(pn.ggplot(coEvents_std, pn.aes('numCoEvents', 'std'))\n",
    " +pn.geom_point()\n",
    " +pn.stat_smooth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.2] Using the function `mpSampleTW` compute the avg uniqueness of each label. What is the first-order serial correlation, `AR(1)` of this time series? Is it statistically significant? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:52:26.546822Z",
     "start_time": "2018-10-18T22:52:26.271531Z"
    }
   },
   "outputs": [],
   "source": [
    "lag = 1\n",
    "lag_col = f'tW_lag_{lag}'\n",
    "out[lag_col] = out['tW'].shift(lag)\n",
    "cprint(out.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:08.404214Z",
     "start_time": "2018-10-18T22:52:26.548177Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl:\n",
    "    pm.GLM.from_formula(f'tW ~ {lag_col}', out.dropna())\n",
    "    trace = pm.sample(3000, cores=1, nuts_kwargs={'target_accept':0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:09.805949Z",
     "start_time": "2018-10-18T22:53:08.406295Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plot_traces(trace, retain=1_000)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:10.174761Z",
     "start_time": "2018-10-18T22:53:09.808051Z"
    }
   },
   "outputs": [],
   "source": [
    "df_smry = pm.summary(trace[1000:])\n",
    "df_smry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order correlation between `tW` and `tW lag 1` appears statistically significant as all the mass of the distribution is nonzero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.3] Fit a random forest to a financial dataset where $I^{-1}\\sum_{i=1}^{I}\\bar u \\ll 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the mean out of bag accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:11.215969Z",
     "start_time": "2018-10-18T22:53:10.177824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "Xy = (pd.DataFrame()\n",
    "      .assign(close=close,\n",
    "              close_lag=close.shift(1))\n",
    "     ).dropna()\n",
    "\n",
    "y = Xy.loc[:,'close'].values\n",
    "X = Xy.loc[:,'close_lag'].values.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "n_estimator = 50\n",
    "rf = RandomForestRegressor(max_depth=1, n_estimators=n_estimator,\n",
    "                           criterion='mse', oob_score=True,\n",
    "                           random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:11.536455Z",
     "start_time": "2018-10-18T22:53:11.217766Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What is the mean accuracy of k-fold cross-validation (without shuffling) on the same dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:13.237222Z",
     "start_time": "2018-10-18T22:53:11.537897Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "n_estimator = 50\n",
    "rf = RandomForestRegressor(max_depth=1, n_estimators=n_estimator,\n",
    "                           criterion='mse', oob_score=True,\n",
    "                           random_state=RANDOM_STATE)\n",
    "\n",
    "scores = cross_validate(rf, X, y, cv=5, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:13.506700Z",
     "start_time": "2018-10-18T22:53:13.239084Z"
    }
   },
   "outputs": [],
   "source": [
    "oob_scores = [est.oob_score_ for est in scores['estimator']]\n",
    "oob_scores, np.mean(oob_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which one is more correct / less biased? What is the source of this bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of bag accuracy is higher than cross-validation b/c the incorrect assumption of IID draws leads to oversampling of redudant samples. \n",
    "\n",
    "For random forests this means that the trees too similar. The random sampling also means that in-bag and out-of-bag samples will be similar inflating the `oob_score_`. In this example the cross-validation is less-biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the code in Section 4.7 to apply an exponential time-decay factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:14.121337Z",
     "start_time": "2018-10-18T22:53:13.508787Z"
    }
   },
   "outputs": [],
   "source": [
    "def getExTimeDecay(tW,clfLastW=1.,exponent=1):\n",
    "    # apply exponential decay to observed uniqueness (tW)\n",
    "    # newest observation gets weight=1, oldest observation gets weight=clfLastW\n",
    "    clfW=tW.sort_index().cumsum()\n",
    "    if clfLastW>=0: slope=((1.-clfLastW)/clfW.iloc[-1])**exponent\n",
    "    else: slope=(1./((clfLastW+1)*clfW.iloc[-1]))**exponent\n",
    "    const=1.-slope*clfW.iloc[-1]\n",
    "    clfW=const+slope*clfW\n",
    "    clfW[clfW<0]=0\n",
    "    print(round(const,4), round(slope,4))\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:53:14.806513Z",
     "start_time": "2018-10-18T22:53:14.125690Z"
    }
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,figsize=(10,7))\n",
    "fs = [1,.75,.5,0,-.25,-.5]\n",
    "ls = ['-','-.','--',':','--','-.']\n",
    "for lstW, l in zip(fs,ls):\n",
    "    decayFactor = getExTimeDecay(out['tW'].dropna(), \n",
    "                                 clfLastW=lstW,\n",
    "                                 exponent=0.75) # experiment by changing exponent\n",
    "    ((out['w'].dropna()*decayFactor).reset_index(drop=True)\n",
    "     .plot(ax=ax[0],alpha=0.5))\n",
    "    s = (pd.Series(1,index=out['w'].dropna().index)*decayFactor)\n",
    "    s.plot(ax=ax[1], ls=l, label=str(lstW))\n",
    "ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider you have applied meta-labels to events determined by a trend-following model. Suppose `2/3` of labels are `0` and `1/3` are `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What happens if you fit a classifier without balancing class weights?\n",
    "\n",
    "The classifier will maximize accuracy by over predicting the dominant class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) A label `1` means true positive and a label `0` means a false positive. By applying balanced class weights, we are forcing the classifier to pay more attention to the true positives, and less attention to the false positives. Why does that make sense?\n",
    "\n",
    "Tying the output to real-life purpose means that too many false positives result in bad trades/investments which means lost capital. From a ML perspective without balanced class weights we will maximize accuracy by simply predicting the dominant class. We need to improve `precision: TP/(TP+FP)` relative to `recall: TP/(TP+FN)` not just overall accuracy `(TP+TN)/(TP+FP+TN+FN)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) What is the distribution of the predicted labels, before and after applying balanced class weights?\n",
    "\n",
    "Before balanced class weights is an unbalanced or skewed distribution.\n",
    "After balanced class weights predicted labels would be more evenly distributed depending on the predictive power of the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the draw probabilities for the final draw in section `4.5.3.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is mencioned in the 4.5.3 numerical example, the first number drawn and included in $\\phi$ is 2 from the set {1, 2, 3} correponding to the columns heading  $\\to \\phi^{1}=[2]$. This number selection was carried out with a uniform probability of $\\delta_{i}=\\frac{1}{3}$.\n",
    "\n",
    "To calculate the probabilities of the next draw it is necessary to calculate the average uniqueness of the features (or possible numbers) taking into account the previous draw.\n",
    "\n",
    "It is important to mention that the set of the numbers to be drawn {1, 2, 3} corresponds to the columns heading [0, 1, 2] of the matrix indM.\n",
    "\n",
    "Before presenting the calculation let's summarize the mathematical expressions composing the average uniqueness:\n",
    "\n",
    "$\\bar{u}_i = (\\sum_{t=1}^{T}u_{t,1})(\\sum_{t=1}^{T}1_{t,i})^{-1}$\n",
    "\n",
    "$u_{t,i}=1_{t,i} c^{-1}$\n",
    "\n",
    "$c = \\sum_{i=1}^{I} 1_{t,i}$\n",
    "\n",
    "then,   \n",
    "$u_{t,i}=1_{t,i}\\sum_{i=1}^{I} 1_{t,i}$\n",
    "\n",
    "\n",
    "Knowing that $indM = \\begin{bmatrix}\n",
    "                     \\underline{\\mathbf{\\color{red}1}} & \\underline{\\mathbf{\\color{red}2}} & \\underline{\\mathbf{\\color{red}3}}\\\\\n",
    "                      1 & 0 & 0 \\\\\n",
    "                      1 & 0 & 0 \\\\\n",
    "                      1 & 1 & 0 \\\\\n",
    "                      0 & 1 & 0 \\\\\n",
    "                      0 & 0 & 1 \\\\\n",
    "                      0 & 0 & 1 \n",
    "                      \\end{bmatrix}$\n",
    "\n",
    "\n",
    "The numbers of the example developed in the book comes from:\n",
    "\n",
    ">$\\bar{u}_{i}^{(2)} = (1 + 1 + 1) \\frac{1}{3}$\n",
    "\n",
    "where \n",
    "\n",
    "> $\\bar{u}_{i}^{(2)} = (\\sum_{t=1}^{T}u_{t,1})(\\sum_{t=1}^{T}1_{t,i})^{-1}$\n",
    "\n",
    "> $u_{t,i}^{(2)}=1_{t,i}(1+\\sum_{k\\in\\phi^{(1)}}1_{t,k})^{-1}$\n",
    "\n",
    "\n",
    "$u_{t,1}^{(2)}=\\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix} (1+\\sum_{k\\in\\phi^{(1)}} \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix} (1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}\\div\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\1\\\\1\\end{bmatrix} = \\begin{bmatrix}1\\\\1\\\\\\frac{1}{2}\\\\0\\\\0\\\\0\\end{bmatrix}$\n",
    "\n",
    "With this value we can calculate the average uniqueness of the first feature:\n",
    "\n",
    ">$\\bar{u}_i = (\\sum_{t=1}^{T}u_{t,1})(\\sum_{t=1}^{T}1_{t,i})^{-1}$\n",
    "\n",
    ">$\\bar{u}_{1}^{(2)}=(1 + 1 + \\frac{1}{2})\\div(1 + 1 + 1)=\\frac{\\frac{5}{2}}{3}=\\frac{5}{6}$\n",
    "                \n",
    "    \n",
    "    \n",
    "**feature 2:**\n",
    "\n",
    "$u_{t,2}^{(2)}=\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix} (1+\\sum_{k\\in\\phi^{(1)}} \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix} (1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix}\\div\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\1\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\\\frac{1}{2}\\\\\\frac{1}{2}\\\\0\\\\0\\end{bmatrix}$\n",
    "\n",
    "                \n",
    ">$\\bar{u}_{2}^{(2)}=(\\frac{1}{2} + \\frac{1}{2})\\div(1 + 1)=\\frac{1}{2}$\n",
    "                                \n",
    "\n",
    "**feature 3:**\n",
    "\n",
    "$u_{t,3}^{(2)}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix} (1+\\sum_{k\\in\\phi^{(1)}} \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix} (1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix})^{-1} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix}\\div\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\1\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix}$\n",
    "\n",
    "                \n",
    ">$\\bar{u}_{3}^{(2)}=(1 + 1)\\div(1 + 1)=1$\n",
    "                                \n",
    "\n",
    "Having calculated all the average uniqueness values the next step is to calculate the probability:\n",
    "\n",
    ">$\\delta_{j}^{(2)}=\\bar{u}_{j}^{(2)}(\\sum_{k=1}^{I}\\bar{u}_{k}^{(2)})^{-1}$\n",
    "\n",
    ">$\\sum_{k=1}^{I}\\bar{u}_{k}^{(2)}=\\frac{5}{6}+\\frac{1}{2}+1=\\frac{5}{6}+\\frac{3}{6}+\\frac{6}{6}=\\frac{14}{6}=\\frac{7}{3}$\n",
    "\n",
    ">$\\delta_{1}^{(2)}=\\frac{\\frac{5}{6}}{\\frac{7}{3}}=\\frac{5}{14}=0.3571$\n",
    "\n",
    ">$\\delta_{2}^{(2)}=\\frac{\\frac{1}{2}}{\\frac{7}{3}} = \\frac{3}{14}=0.2143$\n",
    "\n",
    ">$\\delta_{3}^{(2)}=\\frac{1}{\\frac{7}{3}}=\\frac{3}{7}=\\frac{6}{14}=0.4286$\n",
    "\n",
    ">$\\sum\\delta^{(2)}=\\frac{5}{14}+\\frac{3}{14}+\\frac{6}{14}=\\frac{14}{14}=1$\n",
    "\n",
    "Notes from the book:\n",
    "\n",
    "1) The feature already picked gets the lowest probability due to the overlap with itself would be highest. \n",
    "\n",
    "2) The third feature gets the highest probability because it has no overlap with the second feature which was the picked one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous results can be obtained with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.Series([2,3,5],index=[0,2,4])\n",
    "barIx=range(t1.max()+1) # index of bars\n",
    "indM=getIndMatrix(barIx,t1)\n",
    "indM     \n",
    "# The columns heading correspond to the feature set {1, 2, 3} respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = [1]        # phi = [1] corresponds to column 1, feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following statements are taken from the SNIPPET 4.5 - function seqBootstrap(indM,sLength=None)\n",
    "\n",
    "avgU=pd.Series()\n",
    "for i in indM:\n",
    "    indM_=indM[phi+[i]] # reduce indM\n",
    "    avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]\n",
    "\n",
    "print('Average Uniqueness: \\n',avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = avgU/avgU.sum()\n",
    "print('Feature draw probabilities: \\n', prob1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical example continues selecting 3 as the second draw.\n",
    "\n",
    "$\\phi = [2, 3]$\n",
    "\n",
    "**first feature:**\n",
    "\n",
    "$u_{t,1}=1_{t,1}(1+\\sum_{k\\in\\phi}1_{t,k})^{-1}$\n",
    "\n",
    "$u_{t,1}=\\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}\\times(1+\\sum\\begin{bmatrix}0 & 0\\\\0 & 0\\\\1 & 0\\\\1 & 0\\\\0 & 1\\\\0 & 1\\end{bmatrix})^{-1}$\n",
    "\n",
    "$u_{t,1}=\\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}\\times(1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\1\\end{bmatrix})^{-1}=\\begin{bmatrix}1\\\\1\\\\1\\\\0\\\\0\\\\0\\end{bmatrix}\\times\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\2\\\\2\\end{bmatrix}^{-1}=\\begin{bmatrix}1\\\\1\\\\\\frac{1}{2}\\\\0\\\\0\\\\0\\end{bmatrix}$\n",
    "\n",
    "$\\bar{u}_{1}^{(2,3)}=(\\sum_{t=1}^{T}u_{t,1})(\\sum_{t,1}^{T}1_{t,1})^{-1} = \\frac{(1+1+\\frac{1}{2})}{(1+1+1)}=\\frac{\\frac{2}{2}+\\frac{2}{2}+\\frac{1}{2}}{3}$\n",
    "\n",
    "$\\bar{u}_{1}^{(2,3)} = \\frac{5}{6}$\n",
    "\n",
    "**second feature:**\n",
    "\n",
    "$u_{t,2}=1_{t,2}(1+\\sum_{k\\in\\phi}1_{t,k})^{-1}$\n",
    "\n",
    "$u_{t,2}=\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix}\\times(1+\\sum\\begin{bmatrix}0 & 0\\\\0 & 0\\\\1 & 0\\\\1 & 0\\\\0 & 1\\\\0 & 1\\end{bmatrix})^{-1}$\n",
    "\n",
    "$u_{t,2}=\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix}\\times(1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\1\\end{bmatrix})^{-1}=\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\0\\\\0\\end{bmatrix}\\times\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\2\\\\2\\end{bmatrix}^{-1}=\\begin{bmatrix}0\\\\0\\\\\\frac{1}{2}\\\\\\frac{1}{2}\\\\0\\\\0\\end{bmatrix}$\n",
    "\n",
    "$\\bar{u}_{2}^{(2,3)}=(\\sum_{t=1}^{T}u_{t,2})(\\sum_{t,1}^{T}1_{t,2})^{-1} = \\frac{(\\frac{1}{2}+\\frac{1}{2})}{(1+1)}=\\frac{1}{2}$\n",
    "\n",
    "**third feature:**\n",
    "\n",
    "$u_{t,3}=1_{t,3}(1+\\sum_{k\\in\\phi}1_{t,k})^{-1}$\n",
    "\n",
    "$u_{t,3}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix}\\times(1+\\sum\\begin{bmatrix}0 & 0\\\\0 & 0\\\\1 & 0\\\\1 & 0\\\\0 & 1\\\\0 & 1\\end{bmatrix})^{-1}$\n",
    "\n",
    "$u_{t,3}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix}\\times(1+\\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\1\\end{bmatrix})^{-1}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\1\\\\1\\end{bmatrix}\\times\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\2\\\\2\\end{bmatrix}^{-1}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\\\frac{1}{2}\\\\\\frac{1}{2}\\end{bmatrix}$\n",
    "\n",
    "$\\bar{u}_{3}^{(2,3)}=(\\sum_{t=1}^{T}u_{t,3})(\\sum_{t,1}^{T}1_{t,3})^{-1} = \\frac{(\\frac{1}{2}+\\frac{1}{2})}{(1+1)}=\\frac{1}{2}$\n",
    "\n",
    "Once the average uniqueness of all features has been calculated the next step is to get the probabilities.\n",
    "\n",
    ">$\\delta_{j}^{(2,3)}=\\bar{u}_{j}^{(2,3)}(\\sum_{k=1}^{(2,3)})^{-1}$\n",
    "\n",
    ">$\\delta_{j}^{(2,3)}=\\frac{(\\frac{5}{6}, \\frac{1}{2}, \\frac{1}{2})}{(\\frac{5}{6}+\\frac{1}{2}+\\frac{1}{2})}=\\frac{(\\frac{5}{6}, \\frac{1}{2}, \\frac{1}{2})}{\\frac{11}{6}}$\n",
    "\n",
    ">$\\delta_{j}^{(2,3)}=(\\frac{5}{11}, \\frac{3}{11}, \\frac{3}{11})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous results can be obtained with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = [1,2]      # These values corresponds to the indM columns or features [2,3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following statements are taken from the SNIPPET 4.5 - function seqBootstrap(indM,sLength=None)\n",
    "avgU=pd.Series()\n",
    "for i in indM:\n",
    "    indM_=indM[phi+[i]] # reduce indM\n",
    "    avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]\n",
    "\n",
    "print('Average Uniqueness: \\n',avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = avgU/avgU.sum()\n",
    "print('Feature draw probabilities: \\n', prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest probability corresponds to the feature with the least overlap due to it has not been selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T21:54:59.730085Z",
     "start_time": "2018-10-17T21:54:59.412835Z"
    }
   },
   "source": [
    "## In Section `4.5.3` suppose that number 2 is picked again in the second draw. Waht would be the updated probabilities for the third draw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number 2 is located in column 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgU=pd.Series()\n",
    "for i in indM:\n",
    "    indM_=indM[phi+[i]] # reduce indM\n",
    "    avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]\n",
    "\n",
    "print('Average Uniqueness: \\n',avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3 = avgU/avgU.sum()\n",
    "print('Feature draw probabilities: \\n', prob3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "343px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "466px",
    "left": "1359.63px",
    "right": "20px",
    "top": "230.994px",
    "width": "325px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
